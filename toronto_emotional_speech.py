# -*- coding: utf-8 -*-
"""Toronto emotional speech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Ki5HNUNEXRnBFa641cquiLixx2Jz68x
"""

!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess

# Unzip downloaded dataset
!unzip toronto-emotional-speech-set-tess.zip

import os
import librosa
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import streamlit as st

# Specify the path to the TESS dataset
dataset_path = "TESS Toronto emotional speech set data/"

# Prepare lists for features and labels
features = []
labels = []

# Iterate through each emotion directory
for emotion in os.listdir(dataset_path):
    emotion_path = os.path.join(dataset_path, emotion)

    if os.path.isdir(emotion_path):
        # Iterate through each audio file in the emotion directory
        for audio_file in os.listdir(emotion_path):
            if audio_file.endswith('.wav'):
                file_path = os.path.join(emotion_path, audio_file)

                # Load the audio file
                y, sr = librosa.load(file_path, sr=22050)

                # Extract MFCCs
                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

                # Take the mean of the MFCCs
                mfccs_mean = np.mean(mfccs.T, axis=0)

                features.append(mfccs_mean)
                labels.append(emotion)

features = np.array(features)
labels = np.array(labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

# Train the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Test the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

# Display results
st.write(f"Accuracy: {accuracy * 100:.2f}%")
st.write("Classification Report:\n", classification_report(y_test, y_pred))

# Emoji mapping for predicted emotions
emotion_emoji = {
    "happy": "üòÄ",
    "sad": "üò¢",
    "angry": "üò°",
    "fear": "üò±",
    "disgust": "ü§¢",
    "surprise": "üò≤",
    "neutral": "üòê"
}

# Streamlit App to test the model
st.title("Speech Emotion Recognition")

uploaded_file = st.file_uploader("Upload a WAV file for emotion recognition", type=["wav"])

if uploaded_file is not None:
    # Save the uploaded file
    file_name = "uploaded_audio.wav"
    with open(file_name, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # Load the uploaded audio file
    y, sr = librosa.load(file_name, sr=22050)

    # Extract MFCCs
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

    # Take the mean of the MFCCs
    mfccs_mean = np.mean(mfccs.T, axis=0)

    # Predict the emotion
    feature = mfccs_mean.reshape(1, -1)
    predicted_emotion = model.predict(feature)[0]

    # Extract the emotion part from the predicted label (e.g., remove 'OAF_' or other prefixes)
    clean_emotion = predicted_emotion.split('_')[-1].lower()  # Take the part after the '_'

    # Get the corresponding emoji for the predicted emotion
    emoji = emotion_emoji.get(clean_emotion, "‚ùì")  # Default to question mark if not found

    # Display predicted emotion with emoji
    st.markdown(f"**Predicted Emotion:** {clean_emotion.capitalize()} {emoji}")
    st.audio(uploaded_file, format="audio/wav")